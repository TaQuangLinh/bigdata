{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65b317f",
   "metadata": {},
   "source": [
    "## train model phân loại nhãn của bình luận "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7076cb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LINH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LINH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\LINH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chạy 2 lệnh này nếu bạn chưa có thư viện\n",
    "# !pip install nltk\n",
    "# !pip install imbalanced-learn\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline # Sẽ dùng cho GridSearch\n",
    "\n",
    "# Tải tài nguyên cho NLTK (chỉ cần chạy 1 lần)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74f04382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng quan dữ liệu:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16102 entries, 0 to 16101\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   airline_sentiment  16102 non-null  object\n",
      " 1   negativereason     10095 non-null  object\n",
      " 2   airline            16102 non-null  object\n",
      " 3   text               16102 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 503.3+ KB\n",
      "None\n",
      "\n",
      "Phân bố cảm xúc:\n",
      "airline_sentiment\n",
      "negative    62.694075\n",
      "neutral     21.165073\n",
      "positive    16.140852\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# cell 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "# Giả sử đây là DataFrame của bạn (trong thực tế, bạn sẽ dùng pd.read_csv)\n",
    "df = pd.read_csv('df_test.csv') \n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# Thêm dữ liệu giả định để mô phỏng sự mất cân bằng bạn đã nêu\n",
    "# (Trong code thực tế, bạn không cần bước này, chỉ cần tải file của bạn)\n",
    "more_data = {\n",
    "    'airline_sentiment': ['negative']*917 + ['neutral']*309 + ['positive']*236,\n",
    "    'negativereason': ['Late Flight']*300 + ['Bad Flight']*300 + [\"Can't Tell\"]*317 + [np.nan]*309 + [np.nan]*236,\n",
    "    'airline': ['Other']*1462,\n",
    "    'text': ['some negative text']*917 + ['some neutral text']*309 + ['some positive text']*236\n",
    "}\n",
    "df_more = pd.DataFrame(more_data)\n",
    "df = pd.concat([df, df_more], ignore_index=True)\n",
    "\n",
    "print(\"Tổng quan dữ liệu:\")\n",
    "print(df.info())\n",
    "print(\"\\nPhân bố cảm xúc:\")\n",
    "print(df['airline_sentiment'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98a97212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang áp dụng tiền xử lý nâng cao...\n",
      "...Hoàn tất tiền xử lý.\n",
      "\n",
      "Ví dụ sau khi làm sạch (Nâng cao):\n",
      "Gốc: @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      "Sau khi làm sạch: really aggressive blast obnoxious entertainment guest face amp little recourse\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Tải tài nguyên cho NLTK (chỉ cần chạy 1 lần nếu đã chạy rồi)\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Khởi tạo các đối tượng\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Bổ sung các từ có thể muốn loại bỏ\n",
    "custom_stopwords = {'virginamerica', 'united', 'americanair', 'jetblue', 'usairways', 'southwestair'}\n",
    "stop_words = stop_words.union(custom_stopwords)\n",
    "\n",
    "\n",
    "def preprocess_text_advanced(text):\n",
    "    \"\"\"\n",
    "    Hàm làm sạch văn bản (Nâng cao):\n",
    "    1. Chuyển về chữ thường\n",
    "    2. Loại bỏ URL\n",
    "    3. Loại bỏ @mentions\n",
    "    4. Loại bỏ dấu câu và số\n",
    "    5. Tokenize (tách từ)\n",
    "    6. Lemmatization (đưa về từ gốc)\n",
    "    7. Loại bỏ Stopwords\n",
    "    8. Ghép lại thành chuỗi\n",
    "    \"\"\"\n",
    "    # Xử lý trường hợp đầu vào không phải là chuỗi (ví dụ: float, NaN)\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    text = text.lower()  # Chuyển về chữ thường\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Loại bỏ URL\n",
    "    text = re.sub(r'\\@\\w+', '', text)  # Loại bỏ @mentions\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))  # Loại bỏ dấu câu và số\n",
    "    \n",
    "    # Tokenize và Lemmatize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # *** SỬA LỖI Ở ĐÂY ***\n",
    "    # Đổi 'lemize' thành 'lemmatize'\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Loại bỏ stopwords\n",
    "    filtered_tokens = [word for word in lemmatized_tokens if word not in stop_words and len(word) > 1]\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Giả sử 'df' là DataFrame gốc của bạn (bạn đã có df = pd.read_csv('df_test.csv'))\n",
    "print(\"Đang áp dụng tiền xử lý nâng cao...\")\n",
    "df['text_cleaned'] = df['text'].apply(preprocess_text_advanced)\n",
    "print(\"...Hoàn tất tiền xử lý.\")\n",
    "\n",
    "# In thử ví dụ sau khi làm sạch\n",
    "print(\"\\nVí dụ sau khi làm sạch (Nâng cao):\")\n",
    "# Kiểm tra xem df có đủ 4 dòng không\n",
    "if len(df) > 3:\n",
    "    print(f\"Gốc: {df['text'].iloc[3]}\")\n",
    "    print(f\"Sau khi làm sạch: {df['text_cleaned'].iloc[3]}\")\n",
    "else:\n",
    "    print(\"Dữ liệu không đủ 4 dòng để in ví dụ iloc[3].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5e9e78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bắt đầu huấn luyện và tìm kiếm tham số Mô hình 1...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "...Huấn luyện hoàn tất.\n",
      "Tham số tốt nhất tìm được: {'svc__C': 0.1}\n",
      "\n",
      "--- Kết quả Mô hình 1 (Cải tiến với GridSearchCV) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.90      0.88      2019\n",
      "     neutral       0.66      0.61      0.64       682\n",
      "    positive       0.75      0.71      0.73       520\n",
      "\n",
      "    accuracy                           0.81      3221\n",
      "   macro avg       0.76      0.74      0.75      3221\n",
      "weighted avg       0.80      0.81      0.81      3221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Giả sử X và y đã được tạo từ 'text_cleaned' mới\n",
    "X = df['text_cleaned']\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 1. Tạo một Pipeline\n",
    "# Pipeline sẽ tự động thực hiện 2 bước:\n",
    "# 'tfidf': Vector hóa văn bản\n",
    "# 'svc': Chạy mô hình LinearSVC\n",
    "pipeline_sentiment = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 2), max_features=10000)),\n",
    "    ('svc', LinearSVC(class_weight='balanced', max_iter=2000, random_state=42, dual=False)) # dual=False nhanh hơn khi n_samples > n_features\n",
    "])\n",
    "\n",
    "# 2. Định nghĩa không gian tham số để tìm kiếm\n",
    "# Chúng ta sẽ yêu cầu GridSearchCV thử 3 giá trị C\n",
    "parameters_sentiment = {\n",
    "    'svc__C': [0.1, 1, 10]  # Thử nghiệm với các giá trị regularization\n",
    "    # Bạn có thể thêm nhiều tham số khác: 'tfidf__max_features': [5000, 10000, 20000]\n",
    "}\n",
    "\n",
    "# 3. Khởi tạo GridSearchCV\n",
    "# cv=3 nghĩa là sử dụng 3-fold cross-validation\n",
    "# n_jobs=-1 sử dụng tất cả CPU để tăng tốc\n",
    "# scoring='f1_weighted' tối ưu theo F1-score\n",
    "grid_search_sentiment = GridSearchCV(pipeline_sentiment, parameters_sentiment, cv=3, n_jobs=-1, scoring='f1_weighted', verbose=2)\n",
    "\n",
    "print(\"\\nBắt đầu huấn luyện và tìm kiếm tham số Mô hình 1...\")\n",
    "grid_search_sentiment.fit(X_train, y_train) # *** DÙNG DỮ LIỆU TRAIN CỦA BẠN ***\n",
    "print(\"...Huấn luyện hoàn tất.\")\n",
    "\n",
    "# 4. Đánh giá mô hình tốt nhất\n",
    "print(f\"Tham số tốt nhất tìm được: {grid_search_sentiment.best_params_}\")\n",
    "\n",
    "best_model_sentiment = grid_search_sentiment.best_estimator_\n",
    "y_pred = best_model_sentiment.predict(X_test) # *** DÙNG DỮ LIỆU TEST CỦA BẠN ***\n",
    "\n",
    "print(\"\\n--- Kết quả Mô hình 1 (Cải tiến với GridSearchCV) ---\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "401df1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model đã được lưu thành công vào 'best_model_sentiment.joblib'.\n"
     ]
    }
   ],
   "source": [
    "# lưu lại model \n",
    "\n",
    "import joblib\n",
    "\n",
    "# Lưu model vào file .joblib\n",
    "joblib.dump(best_model_sentiment, \"best_model_sentiment.joblib\")\n",
    "\n",
    "print(\"Model đã được lưu thành công vào 'best_model_sentiment.joblib'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea429ee",
   "metadata": {},
   "source": [
    "## train model tìm từ khóa chính trong bình luận "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c67dee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước tập train (trước SMOTE): (6870, 5000)\n",
      "Kích thước tập train (sau SMOTE): (20952, 5000)\n",
      "\n",
      "Phân bố nhãn sau SMOTE:\n",
      "negativereason\n",
      "Customer Service Issue         2328\n",
      "Cancelled Flight               2328\n",
      "Bad Flight                     2328\n",
      "Late Flight                    2328\n",
      "Flight Booking Problems        2328\n",
      "Lost Luggage                   2328\n",
      "Flight Attendant Complaints    2328\n",
      "Damaged Luggage                2328\n",
      "longlines                      2328\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bắt đầu huấn luyện Mô hình 2 (Cải tiến với SMOTE)...\n",
      "...Huấn luyện hoàn tất.\n",
      "\n",
      "--- Kết quả Mô hình 2 (Cải tiến với SMOTE) ---\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                 Bad Flight       0.47      0.61      0.53       176\n",
      "           Cancelled Flight       0.68      0.70      0.69       169\n",
      "     Customer Service Issue       0.73      0.66      0.70       582\n",
      "            Damaged Luggage       0.75      0.40      0.52        15\n",
      "Flight Attendant Complaints       0.37      0.40      0.38        96\n",
      "    Flight Booking Problems       0.40      0.56      0.47       106\n",
      "                Late Flight       0.69      0.58      0.63       393\n",
      "               Lost Luggage       0.70      0.72      0.71       145\n",
      "                  longlines       0.26      0.36      0.30        36\n",
      "\n",
      "                   accuracy                           0.62      1718\n",
      "                  macro avg       0.56      0.55      0.55      1718\n",
      "               weighted avg       0.64      0.62      0.62      1718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Lọc dữ liệu (giống như trước)\n",
    "df_neg = df[\n",
    "    (df['airline_sentiment'] == 'negative') & \n",
    "    (df['negativereason'].notna()) & \n",
    "    (df['negativereason'] != \"Can't Tell\")\n",
    "].copy()\n",
    "\n",
    "if df_neg.empty:\n",
    "    print(\"\\nKhông có đủ dữ liệu `negativereason`.\")\n",
    "else:\n",
    "    # 2. Xác định X và y (sử dụng 'text_cleaned' đã được cải tiến)\n",
    "    X_reason = df_neg['text_cleaned']\n",
    "    y_reason = df_neg['negativereason']\n",
    "\n",
    "    # 3. Chia Train và Test\n",
    "    X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reason, y_reason, test_size=0.2, random_state=42, stratify=y_reason)\n",
    "\n",
    "    # 4. Vector hóa (Bước này PHẢI làm trước khi dùng SMOTE)\n",
    "    # Chúng ta học từ vựng (fit) trên tập Train, và áp dụng (transform) cho cả Train và Test\n",
    "    vectorizer_reason = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "    X_train_r_tfidf = vectorizer_reason.fit_transform(X_train_r)\n",
    "    X_test_r_tfidf = vectorizer_reason.transform(X_test_r)\n",
    "\n",
    "    print(f\"Kích thước tập train (trước SMOTE): {X_train_r_tfidf.shape}\")\n",
    "\n",
    "    # 5. Áp dụng SMOTE (CHỈ trên tập training)\n",
    "    smote = SMOTE(random_state=42, k_neighbors=3) # k_neighbors=3 vì có lớp chỉ có ~74 mẫu\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_r_tfidf, y_train_r)\n",
    "\n",
    "    print(f\"Kích thước tập train (sau SMOTE): {X_train_resampled.shape}\")\n",
    "    print(\"\\nPhân bố nhãn sau SMOTE:\")\n",
    "    print(y_train_resampled.value_counts())\n",
    "\n",
    "    # 6. Huấn luyện mô hình (dùng LinearSVC)\n",
    "    # Lần này không cần class_weight='balanced' vì dữ liệu đã cân bằng rồi\n",
    "    model_reason = LinearSVC(max_iter=2000, random_state=42, dual=False)\n",
    "    \n",
    "    print(\"\\nBắt đầu huấn luyện Mô hình 2 (Cải tiến với SMOTE)...\")\n",
    "    model_reason.fit(X_train_resampled, y_train_resampled)\n",
    "    print(\"...Huấn luyện hoàn tất.\")\n",
    "\n",
    "    # 7. Đánh giá mô hình (trên tập Test gốc, không SMOTE)\n",
    "    y_pred_r = model_reason.predict(X_test_r_tfidf)\n",
    "\n",
    "    print(\"\\n--- Kết quả Mô hình 2 (Cải tiến với SMOTE) ---\")\n",
    "    print(classification_report(y_test_r, y_pred_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f01b0660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model đã được lưu thành công vào 'model_reason.joblib'.\n"
     ]
    }
   ],
   "source": [
    "# lưu lại model \n",
    "import joblib\n",
    "\n",
    "# Lưu model vào file .joblib\n",
    "joblib.dump(model_reason, \"model_reason.joblib\")\n",
    "\n",
    "print(\"Model đã được lưu thành công vào 'model_reason.joblib'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
